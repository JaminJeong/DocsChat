"""
DocsChat - ë¬¸ì„œ ê¸°ë°˜ RAG ì±„íŒ… ì„œë¹„ìŠ¤
Streamlit UI + LangChain + ChromaDB

ì§€ì›:
  - ë¬¸ì„œ: TXT, PDF, ì›¹ URL
  - LLM: OpenAI, Anthropic, Google, Ollama
  - ì„ë² ë”©: HuggingFace (ë¬´ë£Œ), OpenAI (ìœ ë£Œ)
"""
import os

import streamlit as st

from config.settings import settings
from core.document_loader import load_uploaded_file, load_web, split_documents
from core.embeddings import get_embeddings, DEFAULT_MODELS as EMB_DEFAULT_MODELS
from core.llm_factory import create_llm, MODEL_OPTIONS, PROVIDER_LABELS
from core.rag_engine import RAGEngine
from core.vector_store import (
    check_connection,
    get_document_count,
    get_vectorstore,
    reset_collection,
)

# â”€â”€ í˜ì´ì§€ ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.set_page_config(
    page_title="DocsChat",
    page_icon="ğŸ“š",
    layout="wide",
    initial_sidebar_state="expanded",
    menu_items={
        "About": "DocsChat - RAG ê¸°ë°˜ ë¬¸ì„œ ì±„íŒ… ì„œë¹„ìŠ¤\nhttps://github.com/DocsChat",
    },
)


# â”€â”€ ìºì‹œëœ ë¦¬ì†ŒìŠ¤ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@st.cache_resource(show_spinner="ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘...")
def _get_cached_embeddings(provider: str, model: str, api_key: str, ollama_url: str):
    """ì„ë² ë”© ëª¨ë¸ì„ ìºì‹±í•©ë‹ˆë‹¤ (HuggingFace ëª¨ë¸ì€ ì²« ì‹¤í–‰ ì‹œ ë‹¤ìš´ë¡œë“œ)."""
    return get_embeddings(
        provider=provider,
        model=model or None,
        api_key=api_key or None,
        ollama_base_url=ollama_url,
    )


@st.cache_resource(show_spinner="ChromaDB ì—°ê²° ì¤‘...")
def _get_cached_vectorstore(
    emb_provider: str,
    emb_model: str,
    emb_api_key: str,
    collection_name: str,
    ollama_url: str,
):
    """ë²¡í„°ìŠ¤í† ì–´ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìºì‹±í•©ë‹ˆë‹¤ (ì„¤ì • ë³€ê²½ ì‹œ ìë™ ì¬ìƒì„±)."""
    embeddings = _get_cached_embeddings(emb_provider, emb_model, emb_api_key, ollama_url)
    return get_vectorstore(
        embeddings=embeddings,
        collection_name=collection_name,
    )


# â”€â”€ ì‚¬ì´ë“œë°” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with st.sidebar:
    st.title("âš™ï¸ ì„¤ì •")

    # â”€â”€ ChromaDB ì—°ê²° ìƒíƒœ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if check_connection():
        st.success("ChromaDB ì—°ê²°ë¨", icon="âœ…")
    else:
        st.error(
            f"ChromaDB ì—°ê²° ì‹¤íŒ¨ ({settings.chroma_host}:{settings.chroma_port})\n\n"
            "ChromaDB ì„œë²„ë¥¼ ì‹œì‘í•˜ì„¸ìš”:\n`docker compose up chromadb -d`",
            icon="âŒ",
        )

    st.divider()

    # â”€â”€ LLM ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    with st.expander("ğŸ¤– LLM ì„¤ì •", expanded=True):
        llm_provider = st.selectbox(
            "ì œê³µì",
            options=list(PROVIDER_LABELS.keys()),
            format_func=lambda x: PROVIDER_LABELS[x],
            index=list(PROVIDER_LABELS.keys()).index(settings.llm_provider)
            if settings.llm_provider in PROVIDER_LABELS
            else 0,
            key="llm_provider",
        )

        default_model = settings.llm_model or MODEL_OPTIONS[llm_provider][0]
        if default_model not in MODEL_OPTIONS[llm_provider]:
            default_model = MODEL_OPTIONS[llm_provider][0]

        llm_model = st.selectbox(
            "ëª¨ë¸",
            options=MODEL_OPTIONS[llm_provider],
            index=MODEL_OPTIONS[llm_provider].index(default_model),
            key="llm_model",
        )

        # API Key (OllamaëŠ” ë¶ˆí•„ìš”)
        if llm_provider != "ollama":
            env_key_map = {
                "openai": "OPENAI_API_KEY",
                "anthropic": "ANTHROPIC_API_KEY",
                "google": "GOOGLE_API_KEY",
            }
            llm_api_key = st.text_input(
                "API Key",
                value=os.getenv(env_key_map.get(llm_provider, ""), ""),
                type="password",
                placeholder="API Keyë¥¼ ì…ë ¥í•˜ì„¸ìš”",
                key="llm_api_key",
            )
        else:
            llm_api_key = ""
            st.info(
                f"Ollama URL: `http://{settings.ollama_host}:{settings.ollama_port}`\n\n"
                "Ollama ì‚¬ìš© ì‹œ: `docker compose --profile ollama up -d`",
                icon="â„¹ï¸",
            )

        temperature = st.slider(
            "Temperature",
            min_value=0.0,
            max_value=1.0,
            value=0.1,
            step=0.1,
            help="ê°’ì´ ë‚®ì„ìˆ˜ë¡ ê²°ì •ì , ë†’ì„ìˆ˜ë¡ ì°½ì˜ì ì¸ ë‹µë³€",
            key="temperature",
        )

    # â”€â”€ ì„ë² ë”© ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    with st.expander("ğŸ”¢ ì„ë² ë”© ì„¤ì •", expanded=False):
        emb_provider = st.selectbox(
            "ì œê³µì",
            options=["huggingface", "openai"],
            format_func=lambda x: {
                "huggingface": "HuggingFace (ë¬´ë£Œ/ë¡œì»¬)",
                "openai": "OpenAI (ìœ ë£Œ)",
            }[x],
            index=0 if settings.embedding_provider == "huggingface" else 1,
            key="emb_provider",
        )

        emb_model_options = {
            "huggingface": [
                "sentence-transformers/all-MiniLM-L6-v2",
                "BAAI/bge-m3",
                "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
            ],
            "openai": [
                "text-embedding-3-small",
                "text-embedding-3-large",
                "text-embedding-ada-002",
            ],
        }

        default_emb_model = settings.embedding_model or EMB_DEFAULT_MODELS.get(emb_provider, "")
        emb_opts = emb_model_options[emb_provider]
        emb_idx = emb_opts.index(default_emb_model) if default_emb_model in emb_opts else 0

        emb_model = st.selectbox(
            "ëª¨ë¸",
            options=emb_opts,
            index=emb_idx,
            key="emb_model",
        )

        emb_api_key = ""
        if emb_provider == "openai":
            emb_api_key = st.text_input(
                "OpenAI API Key (ì„ë² ë”©ìš©)",
                value=os.getenv("OPENAI_API_KEY", ""),
                type="password",
                key="emb_api_key",
            )

        st.caption(
            "âš ï¸ ì„ë² ë”© ì„¤ì •ì„ ë³€ê²½í•˜ë©´ ê¸°ì¡´ì— ì¸ë±ì‹±ëœ ë¬¸ì„œì™€ í˜¸í™˜ë˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. "
            "ë³€ê²½ í›„ ì»¬ë ‰ì…˜ì„ ì´ˆê¸°í™”í•˜ì„¸ìš”."
        )

    # â”€â”€ ê²€ìƒ‰ ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    with st.expander("ğŸ” ê²€ìƒ‰ ì„¤ì •", expanded=False):
        top_k = st.slider(
            "Top-K (ê²€ìƒ‰ ê²°ê³¼ ìˆ˜)",
            min_value=1,
            max_value=15,
            value=5,
            key="top_k",
            help="RAG ê²€ìƒ‰ ì‹œ ê°€ì ¸ì˜¬ ë¬¸ì„œ ì²­í¬ ìˆ˜",
        )

        collection_name = st.text_input(
            "ì»¬ë ‰ì…˜ ì´ë¦„",
            value=settings.chroma_collection,
            key="collection_name",
            help="ChromaDB ì»¬ë ‰ì…˜ ì´ë¦„. ë‹¤ë¥¸ ë¬¸ì„œ ì„¸íŠ¸ëŠ” ë‹¤ë¥¸ ì»¬ë ‰ì…˜ ì´ë¦„ì„ ì‚¬ìš©í•˜ì„¸ìš”.",
        )

        chunk_size = st.number_input(
            "ì²­í¬ í¬ê¸° (ë¬¸ì ìˆ˜)",
            min_value=256,
            max_value=4096,
            value=1000,
            step=256,
            key="chunk_size",
        )

        chunk_overlap = st.number_input(
            "ì²­í¬ ì˜¤ë²„ë© (ë¬¸ì ìˆ˜)",
            min_value=0,
            max_value=512,
            value=200,
            step=50,
            key="chunk_overlap",
        )

    st.divider()

    # â”€â”€ ë¬¸ì„œ ê´€ë¦¬ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    st.subheader("ğŸ“„ ë¬¸ì„œ ê´€ë¦¬")

    uploaded_files = st.file_uploader(
        "íŒŒì¼ ì—…ë¡œë“œ",
        type=["pdf", "txt"],
        accept_multiple_files=True,
        help="PDF, TXT íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”",
    )

    web_url = st.text_input(
        "ì›¹ URL",
        placeholder="https://example.com/page",
        help="í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ì›¹ í˜ì´ì§€ URL",
    )

    col1, col2 = st.columns(2)

    with col1:
        index_btn = st.button(
            "ğŸ“¥ ì¸ë±ì‹±",
            type="primary",
            use_container_width=True,
            help="ì„ íƒí•œ ë¬¸ì„œë¥¼ ChromaDBì— ì €ì¥",
        )

    with col2:
        reset_btn = st.button(
            "ğŸ—‘ï¸ ì´ˆê¸°í™”",
            type="secondary",
            use_container_width=True,
            help="ì»¬ë ‰ì…˜ì˜ ëª¨ë“  ë¬¸ì„œë¥¼ ì‚­ì œ",
        )

    # â”€â”€ ì¸ë±ì‹± ì²˜ë¦¬ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if index_btn:
        if not uploaded_files and not web_url:
            st.warning("íŒŒì¼ ë˜ëŠ” ì›¹ URLì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        else:
            with st.spinner("ë¬¸ì„œë¥¼ ì¸ë±ì‹± ì¤‘..."):
                try:
                    ollama_url = f"http://{settings.ollama_host}:{settings.ollama_port}"
                    vs = _get_cached_vectorstore(
                        emb_provider, emb_model, emb_api_key,
                        collection_name, ollama_url,
                    )

                    all_docs = []

                    # íŒŒì¼ ë¡œë“œ
                    for f in (uploaded_files or []):
                        with st.spinner(f"`{f.name}` ë¡œë”© ì¤‘..."):
                            docs = load_uploaded_file(f.read(), f.name)
                            all_docs.extend(docs)
                            st.caption(f"âœ“ {f.name}: {len(docs)}ê°œ í˜ì´ì§€/ì„¹ì…˜")

                    # ì›¹ URL ë¡œë“œ
                    if web_url:
                        with st.spinner(f"ì›¹ í˜ì´ì§€ ë¡œë”© ì¤‘..."):
                            docs = load_web(web_url)
                            all_docs.extend(docs)
                            st.caption(f"âœ“ {web_url}: {len(docs)}ê°œ ì„¹ì…˜")

                    if all_docs:
                        # ì²­í¬ ë¶„í• 
                        chunks = split_documents(
                            all_docs,
                            chunk_size=chunk_size,
                            chunk_overlap=chunk_overlap,
                        )
                        # ChromaDBì— ì €ì¥
                        vs.add_documents(chunks)

                        total = st.session_state.get("indexed_total", 0) + len(chunks)
                        st.session_state["indexed_total"] = total
                        st.session_state["vectorstore_ready"] = True
                        st.success(f"âœ… {len(chunks)}ê°œ ì²­í¬ ì¸ë±ì‹± ì™„ë£Œ!")

                except Exception as e:
                    st.error(f"ì¸ë±ì‹± ì˜¤ë¥˜: {e}")

    # â”€â”€ ì»¬ë ‰ì…˜ ì´ˆê¸°í™” ì²˜ë¦¬ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if reset_btn:
        try:
            reset_collection(collection_name)
            st.session_state["indexed_total"] = 0
            st.session_state["vectorstore_ready"] = False
            st.session_state["messages"] = []
            # ìºì‹œ ë¬´íš¨í™”
            _get_cached_vectorstore.clear()
            st.success(f"ì»¬ë ‰ì…˜ '{collection_name}'ì´ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
            st.rerun()
        except Exception as e:
            st.error(f"ì´ˆê¸°í™” ì˜¤ë¥˜: {e}")

    # â”€â”€ ë¬¸ì„œ ìƒíƒœ í‘œì‹œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    try:
        ollama_url = f"http://{settings.ollama_host}:{settings.ollama_port}"
        vs = _get_cached_vectorstore(
            emb_provider, emb_model, emb_api_key, collection_name, ollama_url,
        )
        doc_count = get_document_count(vs)
        if doc_count > 0:
            st.info(f"ğŸ“Š ì €ì¥ëœ ì²­í¬: **{doc_count}ê°œ**", icon="ğŸ“š")
        else:
            st.caption("ì•„ì§ ì¸ë±ì‹±ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
    except Exception:
        pass


# â”€â”€ ë©”ì¸ ì˜ì—­ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.title("ğŸ“š DocsChat")
st.caption("ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ê±°ë‚˜ ì›¹ URLì„ ì…ë ¥í•˜ê³ , AIì—ê²Œ ì§ˆë¬¸í•˜ì„¸ìš”")

tab_chat, tab_info = st.tabs(["ğŸ’¬ ì±„íŒ…", "â„¹ï¸ ì‚¬ìš© ë°©ë²•"])


# â”€â”€ ì •ë³´ íƒ­ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with tab_info:
    col1, col2 = st.columns(2)

    with col1:
        st.markdown("""
### ğŸš€ ë¹ ë¥¸ ì‹œì‘

1. **ì‚¬ì´ë“œë°”** ì—ì„œ LLM ì œê³µìì™€ API Keyë¥¼ ì„¤ì •í•˜ì„¸ìš”
2. **íŒŒì¼ì„ ì—…ë¡œë“œ**í•˜ê±°ë‚˜ **ì›¹ URL**ì„ ì…ë ¥í•˜ì„¸ìš”
3. **ğŸ“¥ ì¸ë±ì‹±** ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”
4. **ì±„íŒ… íƒ­**ì—ì„œ ì§ˆë¬¸í•˜ì„¸ìš”!

---

### ğŸ“‹ ì§€ì› í˜•ì‹

| í˜•ì‹ | ì„¤ëª… |
|------|------|
| ğŸ“„ PDF | í…ìŠ¤íŠ¸ ê¸°ë°˜ PDF íŒŒì¼ |
| ğŸ“ TXT | ì¼ë°˜ í…ìŠ¤íŠ¸ íŒŒì¼ |
| ğŸŒ ì›¹ URL | ì •ì  HTML ì›¹ í˜ì´ì§€ |
        """)

    with col2:
        st.markdown("""
### ğŸ¤– ì§€ì› LLM

| ì œê³µì | ëª¨ë¸ | API Key í•„ìš” |
|--------|------|-------------|
| OpenAI | GPT-4o, GPT-4o-mini | âœ… |
| Anthropic | Claude 3.5 Sonnet | âœ… |
| Google | Gemini 1.5 Flash/Pro | âœ… |
| Ollama | Llama3, Mistral ë“± | âŒ (ë¡œì»¬) |

---

### ğŸ”§ ì„ë² ë”© ì œê³µì

| ì œê³µì | ë¹„ìš© | íŠ¹ì§• |
|--------|------|------|
| HuggingFace | ë¬´ë£Œ | ë¡œì»¬ ì‹¤í–‰, API Key ë¶ˆí•„ìš” |
| OpenAI | ìœ ë£Œ | ê³ ì„±ëŠ¥, API í˜¸ì¶œ í•„ìš” |
        """)

    st.divider()
    st.markdown("""
### âš ï¸ ì£¼ì˜ì‚¬í•­

- **ì„ë² ë”© ì¼ê´€ì„±**: ì¸ë±ì‹± ì‹œ ì‚¬ìš©í•œ ì„ë² ë”© ëª¨ë¸ê³¼ ì§ˆë¬¸ ì‹œ ëª¨ë¸ì´ ë™ì¼í•´ì•¼ í•©ë‹ˆë‹¤.
  ì„ë² ë”© ì„¤ì • ë³€ê²½ í›„ì—ëŠ” ë°˜ë“œì‹œ ì»¬ë ‰ì…˜ì„ ì´ˆê¸°í™”(ğŸ—‘ï¸)í•˜ê³  ì¬ì¸ë±ì‹±í•˜ì„¸ìš”.
- **ì›¹ URL**: JavaScriptë¡œ ë Œë”ë§ë˜ëŠ” SPA í˜ì´ì§€ëŠ” ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤ (ì •ì  HTMLë§Œ ì§€ì›).
- **Ollama**: `docker compose --profile ollama up -d` ë¡œ Ollama ì„œë¹„ìŠ¤ë¥¼ ë¨¼ì € ì‹œì‘í•˜ì„¸ìš”.
    """)


# â”€â”€ ì±„íŒ… íƒ­ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with tab_chat:
    # ì±„íŒ… íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”
    if "messages" not in st.session_state:
        st.session_state["messages"] = []

    # ì±„íŒ… íˆìŠ¤í† ë¦¬ í‘œì‹œ
    for msg in st.session_state["messages"]:
        with st.chat_message(msg["role"]):
            st.markdown(msg["content"])
            if msg.get("sources"):
                with st.expander("ğŸ“ ì°¸ê³  ë¬¸ì„œ", expanded=False):
                    for src in msg["sources"]:
                        st.caption(src)

    # ëŒ€í™” ì´ˆê¸°í™” ë²„íŠ¼
    if st.session_state["messages"]:
        if st.button("ğŸ—‘ï¸ ëŒ€í™” ì´ˆê¸°í™”", key="clear_chat"):
            st.session_state["messages"] = []
            st.rerun()

    # ì±„íŒ… ì…ë ¥
    if prompt := st.chat_input("ë¬¸ì„œì— ëŒ€í•´ ì§ˆë¬¸í•˜ì„¸ìš”..."):
        # ì‚¬ìš©ì ë©”ì‹œì§€ í‘œì‹œ
        st.session_state["messages"].append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        # ì–´ì‹œìŠ¤í„´íŠ¸ ì‘ë‹µ ìƒì„±
        with st.chat_message("assistant"):
            try:
                ollama_url = f"http://{settings.ollama_host}:{settings.ollama_port}"

                # ë²¡í„°ìŠ¤í† ì–´ ì—°ê²°
                vs = _get_cached_vectorstore(
                    emb_provider, emb_model, emb_api_key,
                    collection_name, ollama_url,
                )

                # ì¸ë±ì‹±ëœ ë¬¸ì„œ í™•ì¸
                doc_count = get_document_count(vs)
                if doc_count == 0:
                    msg = (
                        "ì•„ì§ ì¸ë±ì‹±ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. "
                        "ì‚¬ì´ë“œë°”ì—ì„œ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê±°ë‚˜ ì›¹ URLì„ ì…ë ¥í•œ í›„ "
                        "**ğŸ“¥ ì¸ë±ì‹±** ë²„íŠ¼ì„ í´ë¦­í•´ì£¼ì„¸ìš”."
                    )
                    st.warning(msg)
                    st.session_state["messages"].pop()  # ì‚¬ìš©ì ë©”ì‹œì§€ ì œê±°
                else:
                    # LLM ìƒì„±
                    llm = create_llm(
                        provider=llm_provider,
                        model=llm_model,
                        api_key=llm_api_key or None,
                        ollama_base_url=ollama_url,
                        temperature=temperature,
                        streaming=True,
                    )

                    # RAG ì—”ì§„ ìƒì„± ë° ìŠ¤íŠ¸ë¦¬ë° ì¿¼ë¦¬
                    rag = RAGEngine(llm=llm, vectorstore=vs)
                    stream, source_docs = rag.stream_query(prompt, top_k=top_k)

                    # ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ë‹µë³€ í‘œì‹œ
                    response = st.write_stream(stream)

                    # ì†ŒìŠ¤ ë¬¸ì„œ í‘œì‹œ
                    source_summaries = []
                    if source_docs:
                        with st.expander("ğŸ“ ì°¸ê³  ë¬¸ì„œ", expanded=False):
                            for i, doc in enumerate(source_docs, 1):
                                source = doc.metadata.get("source", "ì•Œ ìˆ˜ ì—†ìŒ")
                                preview = doc.page_content[:200].replace("\n", " ")
                                summary = f"**[{i}] {source}**: {preview}..."
                                st.caption(summary)
                                source_summaries.append(f"[{i}] {source}: {preview}...")

                    # íˆìŠ¤í† ë¦¬ì— ì €ì¥
                    st.session_state["messages"].append({
                        "role": "assistant",
                        "content": response,
                        "sources": source_summaries,
                    })

            except Exception as e:
                error_msg = f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
                st.error(error_msg)
                # ì—ëŸ¬ ë©”ì‹œì§€ë„ íˆìŠ¤í† ë¦¬ì— ì €ì¥
                st.session_state["messages"].append({
                    "role": "assistant",
                    "content": f"âŒ {error_msg}",
                })
