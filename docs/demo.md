# RAG Chat ë°ëª¨ êµ¬ì¶• ë°©ë²•

> Streamlitê³¼ Gradioë¥¼ í™œìš©í•œ RAG Chat ì„œë¹„ìŠ¤ ë°ëª¨ ê°€ì´ë“œ

---

## 1. Streamlit vs Gradio ë¹„êµ

| í•­ëª© | Streamlit | Gradio |
|------|-----------|--------|
| **ì¥ì ** | í’ë¶€í•œ UI ì»´í¬ë„ŒíŠ¸, ìƒíƒœ ê´€ë¦¬ ìš©ì´ | ë¹ ë¥¸ í”„ë¡œí† íƒ€ì…, ê³µìœ  URL ì œê³µ |
| **ì±„íŒ… UI** | `st.chat_message`, `st.chat_input` | `gr.ChatInterface` |
| **íŒŒì¼ ì—…ë¡œë“œ** | `st.file_uploader` | `gr.File` |
| **ìŠ¤íŠ¸ë¦¬ë°** | âœ… (`st.write_stream`) | âœ… |
| **ì»¤ìŠ¤í„°ë§ˆì´ì§•** | ë†’ìŒ | ì¤‘ê°„ |
| **ê³µìœ ** | URL ê³µìœ  ê°€ëŠ¥ | 1-click ê³µìœ  ë§í¬ ì œê³µ |

---

## 2. Streamlit ê¸°ë°˜ RAG Chat ë°ëª¨

### 2.1 ì„¤ì¹˜

```bash
pip install streamlit langchain langchain-openai langchain-community chromadb pypdf
```

### 2.2 í”„ë¡œì íŠ¸ êµ¬ì¡°

```
docschat/
â”œâ”€â”€ app.py              # Streamlit ë©”ì¸ ì•±
â”œâ”€â”€ rag_engine.py       # RAG ë¡œì§
â”œâ”€â”€ Dockerfile
â””â”€â”€ requirements.txt
```

### 2.3 Streamlit ì•± (app.py)

```python
import streamlit as st
from rag_engine import RAGEngine

st.set_page_config(page_title="DocsChat", page_icon="ğŸ“š", layout="wide")
st.title("ğŸ“š DocsChat - RAG Chat")

# ì‚¬ì´ë“œë°” ì„¤ì •
with st.sidebar:
    st.header("âš™ï¸ ì„¤ì •")
    llm_provider = st.selectbox(
        "LLM ì œê³µì",
        ["OpenAI (GPT-4o)", "Anthropic (Claude)", "Ollama (ë¡œì»¬)", "Google (Gemini)"]
    )
    api_key = st.text_input("API Key", type="password")
    vector_db = st.selectbox("Vector DB", ["Chroma", "Qdrant", "Weaviate"])
    top_k = st.slider("ê²€ìƒ‰ ê²°ê³¼ ìˆ˜ (Top-K)", 1, 10, 5)

    st.header("ğŸ“„ ë¬¸ì„œ ì—…ë¡œë“œ")
    uploaded_files = st.file_uploader(
        "PDF, TXT, MD íŒŒì¼ ì—…ë¡œë“œ",
        type=["pdf", "txt", "md"],
        accept_multiple_files=True
    )
    if uploaded_files and st.button("ë¬¸ì„œ ì¸ë±ì‹±", type="primary"):
        with st.spinner("ì²˜ë¦¬ ì¤‘..."):
            st.session_state.rag = RAGEngine(llm_provider, vector_db, api_key)
            count = st.session_state.rag.index_documents(uploaded_files)
            st.success(f"âœ… {count}ê°œ ì²­í¬ ì¸ë±ì‹± ì™„ë£Œ!")

# ì±„íŒ… íˆìŠ¤í† ë¦¬
if "messages" not in st.session_state:
    st.session_state.messages = []

for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

# ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
if prompt := st.chat_input("ë¬¸ì„œì— ëŒ€í•´ ì§ˆë¬¸í•˜ì„¸ìš”..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)
    with st.chat_message("assistant"):
        if "rag" not in st.session_state:
            st.warning("ë¨¼ì € ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ê³  ì¸ë±ì‹±í•´ì£¼ì„¸ìš”!")
        else:
            response, sources = st.session_state.rag.query(prompt, top_k)
            st.markdown(response)
            if sources:
                with st.expander("ğŸ“ ì°¸ê³  ë¬¸ì„œ"):
                    for s in sources:
                        st.info(s)
            st.session_state.messages.append({"role": "assistant", "content": response})
```

### 2.4 RAG ì—”ì§„ (rag_engine.py)

```python
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.document_loaders import PyPDFLoader, TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain.chains import RetrievalQA
import tempfile, os

class RAGEngine:
    def __init__(self, llm_provider: str, vector_db: str, api_key: str = None):
        self.llm = self._init_llm(llm_provider, api_key)
        self.embeddings = OpenAIEmbeddings(api_key=api_key)
        self.vectorstore = None

    def _init_llm(self, provider, api_key):
        if "OpenAI" in provider:
            return ChatOpenAI(model="gpt-4o", api_key=api_key)
        elif "Anthropic" in provider:
            from langchain_anthropic import ChatAnthropic
            return ChatAnthropic(model="claude-3-5-sonnet-20241022", api_key=api_key)
        elif "Ollama" in provider:
            from langchain_ollama import ChatOllama
            return ChatOllama(model="llama3.2")
        elif "Google" in provider:
            from langchain_google_genai import ChatGoogleGenerativeAI
            return ChatGoogleGenerativeAI(model="gemini-1.5-pro", api_key=api_key)

    def index_documents(self, files) -> int:
        documents = []
        for file in files:
            suffix = f".{file.name.split('.')[-1]}"
            with tempfile.NamedTemporaryFile(delete=False, suffix=suffix) as tmp:
                tmp.write(file.read())
                tmp_path = tmp.name
            loader = PyPDFLoader(tmp_path) if file.name.endswith(".pdf") else TextLoader(tmp_path)
            documents.extend(loader.load())
            os.unlink(tmp_path)

        splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=50)
        chunks = splitter.split_documents(documents)
        self.vectorstore = Chroma.from_documents(chunks, self.embeddings)
        return len(chunks)

    def query(self, question: str, top_k: int = 5):
        chain = RetrievalQA.from_chain_type(
            llm=self.llm,
            retriever=self.vectorstore.as_retriever(search_kwargs={"k": top_k}),
            return_source_documents=True
        )
        result = chain.invoke({"query": question})
        sources = [doc.page_content[:200] for doc in result["source_documents"]]
        return result["result"], sources
```

### 2.5 Dockerfile

```dockerfile
FROM python:3.11-slim
WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt
COPY . .
EXPOSE 8501
CMD ["streamlit", "run", "app.py", "--server.port=8501", "--server.address=0.0.0.0"]
```

---

## 3. Gradio ê¸°ë°˜ RAG Chat ë°ëª¨

### 3.1 ì„¤ì¹˜

```bash
pip install gradio langchain langchain-openai chromadb
```

### 3.2 Gradio ì•± (gradio_app.py)

```python
import gradio as gr
from rag_engine import RAGEngine

rag_engine = None

def init_rag(files, llm_provider, api_key, vector_db):
    global rag_engine
    rag_engine = RAGEngine(llm_provider=llm_provider, vector_db=vector_db, api_key=api_key)
    count = rag_engine.index_documents(files)
    return f"âœ… {len(files)}ê°œ íŒŒì¼, {count}ê°œ ì²­í¬ ì¸ë±ì‹± ì™„ë£Œ!"

def chat(message, history, top_k):
    global rag_engine
    if rag_engine is None:
        return history + [[message, "âŒ ë¨¼ì € ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ê³  ì¸ë±ì‹±í•´ì£¼ì„¸ìš”."]]
    response, sources = rag_engine.query(message, top_k=top_k)
    src_text = "\n\nğŸ“ **ì°¸ê³ :**\n" + "\n".join([f"- {s[:150]}..." for s in sources])
    return history + [[message, response + src_text]]

with gr.Blocks(title="DocsChat", theme=gr.themes.Soft()) as demo:
    gr.Markdown("# ğŸ“š DocsChat - RAG Chat ì„œë¹„ìŠ¤")
    with gr.Row():
        with gr.Column(scale=1):
            llm_dd = gr.Dropdown(
                ["OpenAI (GPT-4o)", "Anthropic (Claude)", "Ollama (ë¡œì»¬)", "Google (Gemini)"],
                label="LLM ì œê³µì", value="OpenAI (GPT-4o)"
            )
            api_key_tb = gr.Textbox(label="API Key", type="password")
            vdb_dd = gr.Dropdown(["Chroma", "Qdrant", "Weaviate"], label="Vector DB", value="Chroma")
            top_k_sl = gr.Slider(1, 10, value=5, label="Top-K")
            file_up = gr.File(file_count="multiple", file_types=[".pdf", ".txt", ".md"])
            idx_btn = gr.Button("ğŸ“¥ ë¬¸ì„œ ì¸ë±ì‹±", variant="primary")
            idx_status = gr.Textbox(label="ìƒíƒœ", interactive=False)
        with gr.Column(scale=2):
            chatbot = gr.Chatbot(height=500, label="ëŒ€í™”")
            msg_tb = gr.Textbox(label="ë©”ì‹œì§€", placeholder="ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”...")
            with gr.Row():
                send_btn = gr.Button("ì „ì†¡", variant="primary")
                clear_btn = gr.Button("ì´ˆê¸°í™”")

    idx_btn.click(init_rag, [file_up, llm_dd, api_key_tb, vdb_dd], [idx_status])
    send_btn.click(chat, [msg_tb, chatbot, top_k_sl], [chatbot]).then(lambda: "", outputs=[msg_tb])
    msg_tb.submit(chat, [msg_tb, chatbot, top_k_sl], [chatbot]).then(lambda: "", outputs=[msg_tb])
    clear_btn.click(lambda: [], outputs=[chatbot])

if __name__ == "__main__":
    demo.launch(server_name="0.0.0.0", server_port=7860)
```

---

## 4. Docker Compose í†µí•©

```yaml
version: '3.8'
services:
  streamlit:
    build: .
    ports:
      - "8501:8501"
    environment:
      - CHROMA_HOST=chromadb
    depends_on:
      - chromadb

  chromadb:
    image: chromadb/chroma:latest
    volumes:
      - chroma_data:/chroma/chroma
    ports:
      - "8000:8000"

volumes:
  chroma_data:
```

---

## 5. í•µì‹¬ ê¸°ëŠ¥ ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] íŒŒì¼ ì—…ë¡œë“œ: PDF, TXT, DOCX, MD ì§€ì›
- [ ] LLM ì„ íƒ: OpenAI, Anthropic, Ollama, Google
- [ ] Vector DB ì„ íƒ: Chroma, Qdrant, Weaviate
- [ ] ë©€í‹°í„´ ì±„íŒ… ì§€ì›
- [ ] ì†ŒìŠ¤ ë¬¸ì„œ í‘œì‹œ (ë‹µë³€ ê·¼ê±°)
- [ ] ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ
- [ ] ëŒ€í™” ì´ˆê¸°í™”

---

## 6. ì°¸ê³  ìë£Œ

- [Streamlit ê³µì‹ ë¬¸ì„œ](https://docs.streamlit.io)
- [Streamlit Chat Elements](https://docs.streamlit.io/develop/api-reference/chat)
- [Gradio ê³µì‹ ë¬¸ì„œ](https://www.gradio.app/docs)
- [Gradio ChatInterface](https://www.gradio.app/docs/gradio/chatinterface)
- [LangChain Streamlit Integration](https://python.langchain.com/docs/integrations/callbacks/streamlit)
